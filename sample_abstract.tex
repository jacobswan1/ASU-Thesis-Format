An important objective of AI is to understand real-world observations and build up interactive communication with people.
The ability to interpret and react to the perception reveals the important necessity of developing such a system across both the modalities of Vision (\textbf{V}) and Language (\textbf{L}). Although there have been massive efforts on various VL tasks, \eg, Image/Video Captioning, Visual Question Answering and Textual Grounding, very few of them focus on building the VL models with increased efficiency under real-world scenarios. The main focus of this dissertation is to comprehensively investigate the very uncharted efficient VL learning, aiming to build lightweight, data-efficient, and real-world applicable VL models.

The proposed studies in this dissertation take three primary aspects into account when it comes to efficient VL,
1). {Data Efficiency}: collecting task-specific annotations is prohibitively expensive and so manual labor is not always attainable. Techniques are developed to assist the VL learning from implicit supervisions, \ie, in a weakly-supervised fashion. 2). Continuing from that, efficient representation learning is further explored with increased scalability, leveraging large image-text corpus without task-specific annotations. In particular, the knowledge distillation technique is studied for generic {Representation Learning} which proves to bring substantial performance gain than the regular representation learning schema.
3). {Architectural Efficiency}. Deploying the VL model on edge devices is notoriously challenging due to their cumbersome architectures. To further extend these advancements to the real world, a novel efficient VL architecture is designed to tackle the inference bottleneck and the inconvenient two-stage training. Extensive discussions have been conducted on several critical aspects that prominently influence the performances of compact VL models. 

% or detector-free built upon the vision transformer architecture. This enables the VL model with great training flexibility and also increased inference speed for their deployment on resource-limited devices.
% (1) Modularized Textual Grounding System, a textual grounding model that conducts the object localization using textual descriptions step-wisely using only image-level captions as weak supervisions.
% (2) SEED, a self-supervised distillation paradigm that propose to learn the visual representations using self-supervised learning and knowledge distillation.
% and (3) DistillVLM, a newly proposed compact Visual-Linguistic (VL) model obtained by VL distillation. DistillVLM exhibits excellent inference speed with competitive performances as a generic VL model on multiple downstream tasks, e.g., visual question answering, image captioning and retrieval. 
% To this end, we envisage the possibility of developing an unified VL model built upon the transformer architecture without object detector, which enables the VL model with great flexibility for their deployment on resource-limited devices.

